{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c5dea33",
   "metadata": {},
   "source": [
    "## Classification Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5579d0",
   "metadata": {},
   "source": [
    "### Imports and Configuration Code\n",
    "\n",
    "1. Imports and Global ConfigurationThis first code block handles importing all necessary libraries from pandas, numpy, and sklearn. It also sets up global configurations, including file paths for input data and output submission, the random seed for reproducibility, and key parameters derived from the original analysis (e.g., PCA variance threshold, number of features to select). Using os.path.join helps make file paths more portable across operating systems. Warnings are suppressed for a cleaner output during execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185028f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Refactored Machine Learning Pipeline for Multi-Output Classification.\n",
    "Local Machine Version - Imports and Configuration.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Removed: from google.colab import drive\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, f1_score, accuracy_score,\n",
    "    confusion_matrix, ConfusionMatrixDisplay\n",
    ")\n",
    "import matplotlib.pyplot as plt # For plotting\n",
    "import seaborn as sns # For enhanced plotting\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# --- Global Configuration ---\n",
    "warnings.filterwarnings('ignore') # Suppress warnings\n",
    "\n",
    "# Define base directory (Adjust if your project structure is different)\n",
    "BASE_DATA_DIR = r\"D:\\Semester10\\_Semester10\\ML\\Project\\Classification Project\\base_dir\"\n",
    "\n",
    "# Verify base directory exists\n",
    "if not os.path.exists(BASE_DATA_DIR):\n",
    "    print(f\"WARNING: Base data directory not found at: {BASE_DATA_DIR}\")\n",
    "    print(\"Please ensure the path is correct for your local machine.\")\n",
    "\n",
    "# Training Data Paths\n",
    "TRAIN_CAT_FILE = os.path.join(BASE_DATA_DIR, \"TRAIN_NEW\", \"TRAIN_CATEGORICAL_METADATA_new.xlsx\")\n",
    "TRAIN_FCM_FILE = os.path.join(BASE_DATA_DIR, \"TRAIN_NEW\", \"TRAIN_FUNCTIONAL_CONNECTOME_MATRICES_new_36P_Pearson.csv\")\n",
    "TRAIN_QUANT_FILE = os.path.join(BASE_DATA_DIR, \"TRAIN_NEW\", \"TRAIN_QUANTITATIVE_METADATA_new.xlsx\")\n",
    "TRAIN_SOLUTIONS_FILE = os.path.join(BASE_DATA_DIR, \"TRAIN_NEW\", \"TRAINING_SOLUTIONS.xlsx\")\n",
    "\n",
    "# Test Data Paths\n",
    "TEST_CAT_FILE = os.path.join(BASE_DATA_DIR, \"TEST\", \"TEST_CATEGORICAL.xlsx\")\n",
    "TEST_FCM_FILE = os.path.join(BASE_DATA_DIR, \"TEST\", \"TEST_FUNCTIONAL_CONNECTOME_MATRICES.csv\")\n",
    "TEST_QUANT_FILE = os.path.join(BASE_DATA_DIR, \"TEST\", \"TEST_QUANTITATIVE_METADATA.xlsx\")\n",
    "\n",
    "# Output Path\n",
    "SUBMISSION_FILE_PATH = os.path.join(BASE_DATA_DIR, 'submission_refactored_local_v1.csv')\n",
    "\n",
    "# Parameters\n",
    "SEED = 42\n",
    "VALIDATION_SPLIT_RATIO = 0.3\n",
    "PCA_EXPLAINED_VARIANCE = 0.95\n",
    "NUM_FEATURES_TO_SELECT = 738"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef199fbf",
   "metadata": {},
   "source": [
    "### Data Loading Function Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c5df3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Loading Function ---\n",
    "\n",
    "def load_dataset(categorical_path, fcm_path, quantitative_path, solutions_path=None):\n",
    "    \"\"\"Loads the specified dataset files into pandas DataFrames.\"\"\"\n",
    "    try:\n",
    "        # Check if files exist before attempting to read\n",
    "        if not os.path.exists(categorical_path): raise FileNotFoundError(f\"Categorical file not found: {categorical_path}\")\n",
    "        if not os.path.exists(fcm_path): raise FileNotFoundError(f\"FCM file not found: {fcm_path}\")\n",
    "        if not os.path.exists(quantitative_path): raise FileNotFoundError(f\"Quantitative file not found: {quantitative_path}\")\n",
    "        if solutions_path and not os.path.exists(solutions_path): raise FileNotFoundError(f\"Solutions file not found: {solutions_path}\")\n",
    "\n",
    "        cat_data = pd.read_excel(categorical_path)\n",
    "        fcm_data = pd.read_csv(fcm_path)\n",
    "        quant_data = pd.read_excel(quantitative_path)\n",
    "        target_data = pd.read_excel(solutions_path) if solutions_path else None\n",
    "        print(f\"Data loaded successfully from paths starting with: {os.path.dirname(categorical_path)}\")\n",
    "        return cat_data, fcm_data, quant_data, target_data\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading data: {e}. Please check file paths in the configuration block and your Google Drive.\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during data loading: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0e4d4b",
   "metadata": {},
   "source": [
    "### Execute Data loading code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f646d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Execute Data Loading ---\n",
    "print(\"--- Loading Training Data ---\")\n",
    "train_cat_raw, train_fcm_raw, train_quant_raw, train_targets_raw = load_dataset(\n",
    "    TRAIN_CAT_FILE, TRAIN_FCM_FILE, TRAIN_QUANT_FILE, TRAIN_SOLUTIONS_FILE\n",
    ")\n",
    "print(\"\\n--- Loading Test Data ---\")\n",
    "test_cat_raw, test_fcm_raw, test_quant_raw, _ = load_dataset(\n",
    "    TEST_CAT_FILE, TEST_FCM_FILE, TEST_QUANT_FILE\n",
    ")\n",
    "\n",
    "# Store test IDs for the final submission file\n",
    "test_ids_final = test_cat_raw['participant_id'].copy()\n",
    "\n",
    "# --- Initial Data Inspection (Training Data) ---\n",
    "print(\"\\n--- Initial Training Data Shapes ---\")\n",
    "print(f\"Categorical: {train_cat_raw.shape}\")\n",
    "print(f\"FCM:         {train_fcm_raw.shape}\")\n",
    "print(f\"Quantitative:{train_quant_raw.shape}\")\n",
    "print(f\"Solutions:   {train_targets_raw.shape}\")\n",
    "\n",
    "print(\"\\n--- Categorical Data Info ---\")\n",
    "train_cat_raw.info()\n",
    "\n",
    "print(\"\\n--- Quantitative Data Info ---\")\n",
    "train_quant_raw.info()\n",
    "\n",
    "print(\"\\n--- Initial Missing Value Check (Training Data) ---\")\n",
    "print(\"Missing values in Categorical:\\n\", train_cat_raw.isnull().sum()[train_cat_raw.isnull().sum() > 0])\n",
    "print(\"\\nMissing values in Quantitative:\\n\", train_quant_raw.isnull().sum()[train_quant_raw.isnull().sum() > 0])\n",
    "print(\"\\nMissing values in Solutions:\\n\", train_targets_raw.isnull().sum().sum()) # Should be 0\n",
    "print(\"\\nMissing values in FCM:\\n\", train_fcm_raw.isnull().sum().sum()) # Should be 0\n",
    "print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c18dc80",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA) Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901e8693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Exploratory Data Analysis ---\n",
    "print(\"--- EDA: Target Variable Distributions ---\")\n",
    "\n",
    "# ADHD Outcome\n",
    "print(\"ADHD Outcome Distribution:\")\n",
    "print(train_targets_raw['ADHD_Outcome'].value_counts())\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x='ADHD_Outcome', data=train_targets_raw, palette='viridis')\n",
    "plt.title('Distribution of ADHD Outcome (0=No, 1=Yes)')\n",
    "plt.xlabel('ADHD Outcome')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Sex Distribution\n",
    "print(\"\\nSex Distribution:\")\n",
    "print(train_targets_raw['Sex_F'].value_counts())\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x='Sex_F', data=train_targets_raw, palette='magma')\n",
    "plt.title('Distribution of Sex (0=Male, 1=Female)')\n",
    "plt.xlabel('Sex (Female)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- EDA: Quantitative Feature Example (Age) ---\")\n",
    "# Distribution of MRI_Track_Age_at_Scan\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(train_quant_raw['MRI_Track_Age_at_Scan'], kde=True, color='skyblue', bins=20)\n",
    "plt.title('Distribution of Age at Scan')\n",
    "plt.xlabel('Age at Scan')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- EDA: Categorical Feature Example (Parent 1 Education) ---\")\n",
    "# Distribution of Barratt_Barratt_P1_Edu\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=train_cat_raw, x='Barratt_Barratt_P1_Edu', palette='coolwarm', order = sorted(train_cat_raw['Barratt_Barratt_P1_Edu'].dropna().unique()))\n",
    "plt.title('Distribution of Parent 1 Education Level')\n",
    "plt.xlabel('Barratt_Barratt_P1_Edu Code')\n",
    "plt.ylabel('Count')\n",
    "# plt.xticks(rotation=45) # Optional rotation if labels overlap\n",
    "plt.show()\n",
    "print(\"Education Codes:\\n3= <7th grade, 6=Jr High, 9=Partial HS, 12=HS Grad, 15=Partial College, 18=College, 21=Grad Degree\")\n",
    "\n",
    "\n",
    "print(\"\\n--- EDA: ADHD Prevalence by Parent 1 Education ---\")\n",
    "# Merge targets with categorical for this analysis\n",
    "temp_eda_df = pd.merge(train_cat_raw[['participant_id', 'Barratt_Barratt_P1_Edu']],\n",
    "                       train_targets_raw[['participant_id', 'ADHD_Outcome']],\n",
    "                       on='participant_id')\n",
    "\n",
    "# Calculate mean ADHD outcome per education level (handles NaNs by default in groupby)\n",
    "adhd_prevalence_by_edu = temp_eda_df.groupby('Barratt_Barratt_P1_Edu')['ADHD_Outcome'].mean() * 100 # As percentage\n",
    "print(\"ADHD Outcome Percentage by Parent 1 Education Level:\")\n",
    "print(adhd_prevalence_by_edu.round(2))\n",
    "\n",
    "# Optional: Visualize this prevalence\n",
    "plt.figure(figsize=(10, 6))\n",
    "adhd_prevalence_by_edu.plot(kind='bar', color='lightcoral')\n",
    "plt.title('ADHD Prevalence (%) by Parent 1 Education Level')\n",
    "plt.xlabel('Barratt_Barratt_P1_Edu Code')\n",
    "plt.ylabel('ADHD Outcome (%)')\n",
    "plt.ylim(0, 100) # Set y-axis from 0 to 100 for percentage\n",
    "plt.grid(axis='y', linestyle='--')\n",
    "plt.show()\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Clean up temporary dataframe\n",
    "del temp_eda_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4985f63a",
   "metadata": {},
   "source": [
    "### Pre-processing Function Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a143f579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Preprocessing Step Functions ---\n",
    "\n",
    "# (PCA Function from previous response)\n",
    "def apply_pca_reduction(fcm_train_df, fcm_test_df, variance_ratio, seed, id_column='participant_id'):\n",
    "    \"\"\"Applies PCA for dimensionality reduction on FCM data.\"\"\"\n",
    "    train_ids = fcm_train_df[id_column]\n",
    "    train_features = fcm_train_df.drop(columns=[id_column])\n",
    "    test_ids = fcm_test_df[id_column]\n",
    "    test_features = fcm_test_df.drop(columns=[id_column])\n",
    "\n",
    "    pca_model = PCA(n_components=variance_ratio, random_state=seed)\n",
    "    fcm_train_reduced = pca_model.fit_transform(train_features)\n",
    "    fcm_test_reduced = pca_model.transform(test_features)\n",
    "\n",
    "    n_components = pca_model.n_components_\n",
    "    print(f\"PCA applied. Reduced FCM features to {n_components} components.\")\n",
    "\n",
    "    pca_col_names = [f'PC_{i+1}' for i in range(n_components)]\n",
    "    fcm_train_pca_df = pd.DataFrame(fcm_train_reduced, index=train_ids.index, columns=pca_col_names)\n",
    "    fcm_train_pca_df.insert(0, id_column, train_ids)\n",
    "\n",
    "    fcm_test_pca_df = pd.DataFrame(fcm_test_reduced, index=test_ids.index, columns=pca_col_names)\n",
    "    fcm_test_pca_df.insert(0, id_column, test_ids)\n",
    "\n",
    "    return fcm_train_pca_df, fcm_test_pca_df, pca_model\n",
    "\n",
    "# (Categorical Encoding Function from previous response)\n",
    "def encode_categorical_features(cat_train_df, cat_test_df, id_column='participant_id'):\n",
    "    \"\"\"Performs one-hot encoding on categorical features.\"\"\"\n",
    "    for df in [cat_train_df, cat_test_df]:\n",
    "        for col in df.select_dtypes(include='int').columns:\n",
    "             if col != id_column:\n",
    "                  df[col] = df[col].astype('category')\n",
    "\n",
    "    train_id_series = cat_train_df[id_column]\n",
    "    test_id_series = cat_test_df[id_column]\n",
    "    train_features = cat_train_df.drop(columns=[id_column])\n",
    "    test_features = cat_test_df.drop(columns=[id_column])\n",
    "\n",
    "    train_encoded = pd.get_dummies(train_features, drop_first=True, dummy_na=False)\n",
    "    bool_cols_train = train_encoded.select_dtypes(include=bool).columns\n",
    "    train_encoded[bool_cols_train] = train_encoded[bool_cols_train].astype(int)\n",
    "\n",
    "    test_encoded = pd.get_dummies(test_features, drop_first=True, dummy_na=False)\n",
    "    bool_cols_test = test_encoded.select_dtypes(include=bool).columns\n",
    "    test_encoded[bool_cols_test] = test_encoded[bool_cols_test].astype(int)\n",
    "\n",
    "    train_cols = train_encoded.columns\n",
    "    test_encoded = test_encoded.reindex(columns=train_cols, fill_value=0)\n",
    "\n",
    "    cat_train_encoded_df = pd.concat([train_id_series, train_encoded], axis=1)\n",
    "    cat_test_encoded_df = pd.concat([test_id_series, test_encoded], axis=1)\n",
    "\n",
    "    print(\"Categorical features encoded and aligned.\")\n",
    "    return cat_train_encoded_df, cat_test_encoded_df, train_cols\n",
    "\n",
    "# (Data Merging Function from previous response)\n",
    "def merge_all_data(cat_data, fcm_data, quant_data, target_data=None, id_column='participant_id'):\n",
    "    \"\"\"Merges categorical, FCM, quantitative, and optionally target dataframes.\"\"\"\n",
    "    merged_df = pd.merge(cat_data, fcm_data, on=id_column, how='inner')\n",
    "    merged_df = pd.merge(merged_df, quant_data, on=id_column, how='inner')\n",
    "    if target_data is not None:\n",
    "        target_cols_to_merge = [id_column, 'ADHD_Outcome', 'Sex_F']\n",
    "        merged_df = pd.merge(merged_df, target_data[target_cols_to_merge], on=id_column, how='inner')\n",
    "    print(f\"Dataframes successfully merged. Resulting shape: {merged_df.shape}\")\n",
    "    return merged_df\n",
    "\n",
    "# (Missing Value Imputation Function from previous response)\n",
    "def impute_missing_values(df, train_means=None, train_modes=None):\n",
    "    \"\"\"Imputes missing values using mean for numeric and mode for categoricals.\"\"\"\n",
    "    initial_na_count = df.isna().sum().sum()\n",
    "    print(f\"Initial check: {initial_na_count} missing values in the current DataFrame.\")\n",
    "    if initial_na_count == 0:\n",
    "        print(\"No missing values to impute.\")\n",
    "        return df, train_means, train_modes\n",
    "\n",
    "    means_used = {}\n",
    "    modes_used = {}\n",
    "\n",
    "    numeric_cols = df.select_dtypes(include=np.number).columns\n",
    "    for col in numeric_cols:\n",
    "        if df[col].isnull().any():\n",
    "            fill_value = train_means.get(col) if train_means is not None else df[col].mean()\n",
    "            if pd.isna(fill_value):\n",
    "                 fill_value = 0\n",
    "                 print(f\"Warning: Mean for column '{col}' was NaN. Filling with 0.\")\n",
    "            df[col].fillna(fill_value, inplace=True)\n",
    "            means_used[col] = fill_value\n",
    "\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    for col in categorical_cols:\n",
    "         if df[col].isnull().any():\n",
    "             if df[col].dtype.name != 'category':\n",
    "                  try:\n",
    "                      df[col] = df[col].astype('category')\n",
    "                  except TypeError:\n",
    "                      print(f\"Warning: Could not convert column {col} to category. Skipping mode imputation.\")\n",
    "                      continue\n",
    "             fill_value = train_modes.get(col) if train_modes is not None else df[col].mode()[0]\n",
    "             df[col].fillna(fill_value, inplace=True)\n",
    "             modes_used[col] = fill_value\n",
    "\n",
    "    final_na_count = df.isna().sum().sum()\n",
    "    print(f\"Imputation complete. Remaining missing values: {final_na_count}\")\n",
    "    if final_na_count > 0:\n",
    "        print(f\"Warning: {final_na_count} missing values could not be imputed.\")\n",
    "\n",
    "    calculated_means = pd.Series(means_used) if train_means is None else None\n",
    "    calculated_modes = pd.Series(modes_used) if train_modes is None else None\n",
    "\n",
    "    return df, calculated_means, calculated_modes\n",
    "\n",
    "# (Feature Scaling Function from previous response)\n",
    "def scale_data(train_data, val_data, test_data):\n",
    "    \"\"\"Scales features using MinMaxScaler.\"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    train_scaled = scaler.fit_transform(train_data)\n",
    "    val_scaled = scaler.transform(val_data)\n",
    "    test_scaled = scaler.transform(test_data)\n",
    "    print(\"Data scaling complete using MinMaxScaler.\")\n",
    "    return train_scaled, val_scaled, test_scaled, scaler\n",
    "\n",
    "# (Feature Selection Function from previous response)\n",
    "def perform_feature_selection(X_train, y_train, k, X_val, X_test):\n",
    "    \"\"\"Selects top K features based on mutual information and combines masks.\"\"\"\n",
    "    print(f\"Performing feature selection to get top ~{k} combined features...\")\n",
    "    n_features = X_train.shape[1]\n",
    "    k_safe = min(k, n_features)\n",
    "    if k_safe < k:\n",
    "        print(f\"Warning: Requested k={k} features, but only {n_features} available. Using k={k_safe}.\")\n",
    "\n",
    "    selector_adhd = SelectKBest(score_func=mutual_info_classif, k=k_safe)\n",
    "    selector_adhd.fit(X_train, y_train['ADHD_Outcome'])\n",
    "    adhd_mask = selector_adhd.get_support()\n",
    "    print(f\"Selected {np.sum(adhd_mask)} features based on ADHD_Outcome.\")\n",
    "\n",
    "    selector_sex = SelectKBest(score_func=mutual_info_classif, k=k_safe)\n",
    "    selector_sex.fit(X_train, y_train['Sex_F'])\n",
    "    sex_mask = selector_sex.get_support()\n",
    "    print(f\"Selected {np.sum(sex_mask)} features based on Sex_F.\")\n",
    "\n",
    "    combined_feature_mask = adhd_mask | sex_mask\n",
    "    num_selected = np.sum(combined_feature_mask)\n",
    "    print(f\"Combined mask selects {num_selected} features.\")\n",
    "\n",
    "    X_train_sel = X_train[:, combined_feature_mask]\n",
    "    X_val_sel = X_val[:, combined_feature_mask]\n",
    "    X_test_sel = X_test[:, combined_feature_mask]\n",
    "\n",
    "    return X_train_sel, X_val_sel, X_test_sel, combined_feature_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d027e0d",
   "metadata": {},
   "source": [
    "### Execute Preprocessing Pipeline Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3327a256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Execute Preprocessing Steps ---\n",
    "\n",
    "# Apply PCA\n",
    "print(\"--- Applying PCA ---\")\n",
    "train_fcm_pca_df, test_fcm_pca_df, pca_model = apply_pca_reduction(\n",
    "    train_fcm_raw, test_fcm_raw, PCA_EXPLAINED_VARIANCE, SEED\n",
    ")\n",
    "print(f\"Train FCM PCA shape: {train_fcm_pca_df.shape}, Test FCM PCA shape: {test_fcm_pca_df.shape}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Encode Categorical\n",
    "print(\"--- Encoding Categorical Features ---\")\n",
    "train_cat_encoded_df, test_cat_encoded_df, train_encoded_cols = encode_categorical_features(\n",
    "    train_cat_raw, test_cat_raw\n",
    ")\n",
    "print(f\"Train Cat Encoded shape: {train_cat_encoded_df.shape}, Test Cat Encoded shape: {test_cat_encoded_df.shape}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Merge Data\n",
    "print(\"--- Merging Data Sources ---\")\n",
    "train_merged = merge_all_data(train_cat_encoded_df, train_fcm_pca_df, train_quant_raw, train_targets_raw)\n",
    "test_merged = merge_all_data(test_cat_encoded_df, test_fcm_pca_df, test_quant_raw)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Impute Missing Values\n",
    "print(\"--- Imputing Missing Values ---\")\n",
    "train_imputed, train_means_used, train_modes_used = impute_missing_values(train_merged)\n",
    "test_imputed, _, _ = impute_missing_values(test_merged, train_means=train_means_used, train_modes=train_modes_used)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Align Columns (Post-Imputation/Merge)\n",
    "print(\"--- Aligning Columns ---\")\n",
    "train_feature_cols = train_imputed.drop(columns=['participant_id', 'ADHD_Outcome', 'Sex_F']).columns\n",
    "test_id_col = test_imputed['participant_id']\n",
    "test_imputed = test_imputed.reindex(columns=list(train_feature_cols), fill_value=0) # Align test to train feature cols\n",
    "test_imputed.insert(0, 'participant_id', test_id_col) # Add ID back\n",
    "print(\"Columns aligned.\")\n",
    "print(f\"Final Train imputed shape: {train_imputed.shape}, Final Test imputed shape: {test_imputed.shape}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Prepare Features (X) and Targets (y)\n",
    "print(\"--- Preparing Features and Targets ---\")\n",
    "X_all = train_imputed.drop(columns=['participant_id', 'ADHD_Outcome', 'Sex_F'])\n",
    "y_all = train_imputed[['ADHD_Outcome', 'Sex_F']]\n",
    "X_test_prepared = test_imputed.drop(columns=['participant_id'])\n",
    "\n",
    "# Ensure column names are strings\n",
    "X_all.columns = X_all.columns.astype(str)\n",
    "X_test_prepared.columns = X_test_prepared.columns.astype(str)\n",
    "print(f\"Prepared X_all shape: {X_all.shape}, y_all shape: {y_all.shape}, X_test_prepared shape: {X_test_prepared.shape}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Split Data into Training and Validation Sets\n",
    "print(\"--- Splitting Data ---\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_all, y_all, test_size=VALIDATION_SPLIT_RATIO, random_state=SEED, stratify=y_all['Sex_F']\n",
    ")\n",
    "print(f\"Training set shape: {X_train.shape}, Validation set shape: {X_val.shape}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Scale Features\n",
    "print(\"--- Scaling Features ---\")\n",
    "X_train_s, X_val_s, X_test_s, scaler_model = scale_data(X_train.values, X_val.values, X_test_prepared.values)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Select Features\n",
    "print(\"--- Selecting Features ---\")\n",
    "X_train_sel, X_val_sel, X_test_sel, feature_selection_mask = perform_feature_selection(\n",
    "    X_train_s, y_train, NUM_FEATURES_TO_SELECT, X_val_s, X_test_s\n",
    ")\n",
    "print(f\"Shape after feature selection - Train: {X_train_sel.shape}, Val: {X_val_sel.shape}, Test: {X_test_sel.shape}\")\n",
    "print(\"-\" * 50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f24112",
   "metadata": {},
   "source": [
    "### Model Training, Evaluation, Submission Functions Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c624ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Training, Evaluation, Submission Functions ---\n",
    "\n",
    "# (Model Training Function from previous response)\n",
    "def train_final_model(X_train_final, y_train_final):\n",
    "    \"\"\"Trains the final MultiOutput Logistic Regression model.\"\"\"\n",
    "    base_classifier = LogisticRegression(\n",
    "        C=0.1,\n",
    "        penalty='elasticnet',\n",
    "        solver='saga',\n",
    "        l1_ratio=0.5,\n",
    "        class_weight='balanced',\n",
    "        random_state=SEED,\n",
    "        max_iter=1500\n",
    "    )\n",
    "    multi_output_model = MultiOutputClassifier(base_classifier, n_jobs=-1)\n",
    "    print(\"Training final model (Tuned Logistic Regression)...\")\n",
    "    multi_output_model.fit(X_train_final, y_train_final)\n",
    "    print(\"Model training complete.\")\n",
    "    return multi_output_model\n",
    "\n",
    "# (Model Evaluation Function from previous response - updated to include plots)\n",
    "def evaluate_model(model, X_val_final, y_val_final):\n",
    "    \"\"\"Evaluates the model on the validation set and prints metrics/plots.\"\"\"\n",
    "    print(\"\\n--- Evaluating model on validation set ---\")\n",
    "    y_pred_val = model.predict(X_val_final)\n",
    "    y_pred_adhd_val = y_pred_val[:, 0]\n",
    "    y_pred_sex_val = y_pred_val[:, 1]\n",
    "    y_true_adhd_val = y_val_final['ADHD_Outcome'].values\n",
    "    y_true_sex_val = y_val_final['Sex_F'].values\n",
    "\n",
    "    print(\"Validation Report for ADHD_Outcome:\")\n",
    "    print(classification_report(y_true_adhd_val, y_pred_adhd_val))\n",
    "    print(\"Validation Report for Sex_F:\")\n",
    "    print(classification_report(y_true_sex_val, y_pred_sex_val))\n",
    "\n",
    "    adhd_f1 = f1_score(y_true_adhd_val, y_pred_adhd_val)\n",
    "    sex_f1 = f1_score(y_true_sex_val, y_pred_sex_val)\n",
    "    adhd_acc = accuracy_score(y_true_adhd_val, y_pred_adhd_val)\n",
    "    sex_acc = accuracy_score(y_true_sex_val, y_pred_sex_val)\n",
    "\n",
    "    print(f\"Validation ADHD Accuracy: {adhd_acc:.4f}, F1-Score: {adhd_f1:.4f}\")\n",
    "    print(f\"Validation Sex Accuracy: {sex_acc:.4f}, F1-Score: {sex_f1:.4f}\")\n",
    "\n",
    "    # Plot Confusion Matrices\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    fig.suptitle('Validation Set Confusion Matrices')\n",
    "\n",
    "    # ADHD Confusion Matrix\n",
    "    cm_adhd = confusion_matrix(y_true_adhd_val, y_pred_adhd_val)\n",
    "    disp_adhd = ConfusionMatrixDisplay(confusion_matrix=cm_adhd, display_labels=[0, 1])\n",
    "    disp_adhd.plot(ax=axes[0], cmap='Blues')\n",
    "    axes[0].set_title('ADHD Outcome Confusion Matrix')\n",
    "\n",
    "    # Sex Confusion Matrix\n",
    "    cm_sex = confusion_matrix(y_true_sex_val, y_pred_sex_val)\n",
    "    disp_sex = ConfusionMatrixDisplay(confusion_matrix=cm_sex, display_labels=[0, 1])\n",
    "    disp_sex.plot(ax=axes[1], cmap='Greens')\n",
    "    axes[1].set_title('Sex (Female) Confusion Matrix')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to prevent title overlap\n",
    "    plt.show()\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# (Submission Generation Function from previous response)\n",
    "def generate_submission(model, X_test_final, test_ids, output_path):\n",
    "    \"\"\"Generates predictions on the test set and saves the submission file.\"\"\"\n",
    "    print(\"Generating predictions for the test set...\")\n",
    "    test_preds = model.predict(X_test_final)\n",
    "\n",
    "    submission_output = pd.DataFrame({\n",
    "        'participant_id': test_ids,\n",
    "        'ADHD_Outcome': test_preds[:, 0],\n",
    "        'Sex_F': test_preds[:, 1]\n",
    "    })\n",
    "\n",
    "    submission_output['ADHD_Outcome'] = submission_output['ADHD_Outcome'].astype(int)\n",
    "    submission_output['Sex_F'] = submission_output['Sex_F'].astype(int)\n",
    "\n",
    "    try:\n",
    "        submission_output.to_csv(output_path, index=False)\n",
    "        print(f\"Submission file successfully saved to: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving submission file: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397742d1",
   "metadata": {},
   "source": [
    "### Model Exploarion Setup code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05faaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Exploration Setup ---\n",
    "\n",
    "# Import necessary classifiers and evaluation tools if not already imported\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, f1_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd # Ensure pandas is imported for the results summary\n",
    "\n",
    "# Dictionary to store evaluation results\n",
    "evaluation_results = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1010ddb0",
   "metadata": {},
   "source": [
    "### KNN Model Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97723cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- K-Nearest Neighbors (KNN) ---\n",
    "model_name = 'KNN'\n",
    "print(f\"\\n--- Training and Evaluating: {model_name} ---\")\n",
    "\n",
    "# Define the model\n",
    "knn_model = MultiOutputClassifier(\n",
    "    KNeighborsClassifier(n_neighbors=5, n_jobs=-1) # Using default neighbors=5\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "knn_model.fit(X_train_sel, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred_val_knn = knn_model.predict(X_val_sel)\n",
    "y_pred_adhd_val = y_pred_val_knn[:, 0]\n",
    "y_pred_sex_val = y_pred_val_knn[:, 1]\n",
    "y_true_adhd_val = y_val['ADHD_Outcome'].values\n",
    "y_true_sex_val = y_val['Sex_F'].values\n",
    "\n",
    "# Print Classification Reports\n",
    "print(f\"\\nValidation Report for ADHD_Outcome ({model_name}):\")\n",
    "print(classification_report(y_true_adhd_val, y_pred_adhd_val))\n",
    "print(f\"Validation Report for Sex_F ({model_name}):\")\n",
    "print(classification_report(y_true_sex_val, y_pred_sex_val))\n",
    "\n",
    "# Calculate and store metrics\n",
    "adhd_f1 = f1_score(y_true_adhd_val, y_pred_adhd_val)\n",
    "sex_f1 = f1_score(y_true_sex_val, y_pred_sex_val)\n",
    "adhd_acc = accuracy_score(y_true_adhd_val, y_pred_adhd_val)\n",
    "sex_acc = accuracy_score(y_true_sex_val, y_pred_sex_val)\n",
    "evaluation_results[model_name] = {\n",
    "    'ADHD_Accuracy': adhd_acc, 'ADHD_F1': adhd_f1,\n",
    "    'Sex_Accuracy': sex_acc, 'Sex_F1': sex_f1\n",
    "}\n",
    "print(f\"{model_name} - ADHD Accuracy: {adhd_acc:.4f}, F1-Score: {adhd_f1:.4f}\")\n",
    "print(f\"{model_name} - Sex Accuracy: {sex_acc:.4f}, F1-Score: {sex_f1:.4f}\")\n",
    "\n",
    "# Plot Confusion Matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "fig.suptitle(f'{model_name} - Validation Set Confusion Matrices')\n",
    "cm_adhd = confusion_matrix(y_true_adhd_val, y_pred_adhd_val)\n",
    "disp_adhd = ConfusionMatrixDisplay(confusion_matrix=cm_adhd, display_labels=[0, 1])\n",
    "disp_adhd.plot(ax=axes[0], cmap='Blues')\n",
    "axes[0].set_title('ADHD Outcome')\n",
    "cm_sex = confusion_matrix(y_true_sex_val, y_pred_sex_val)\n",
    "disp_sex = ConfusionMatrixDisplay(confusion_matrix=cm_sex, display_labels=[0, 1])\n",
    "disp_sex.plot(ax=axes[1], cmap='Greens')\n",
    "axes[1].set_title('Sex (Female)')\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n",
    "print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf12641",
   "metadata": {},
   "source": [
    "### Random Forest Model code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947ab212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Random Forest ---\n",
    "model_name = 'RandomForest'\n",
    "print(f\"\\n--- Training and Evaluating: {model_name} ---\")\n",
    "\n",
    "# Define the model\n",
    "rf_model = MultiOutputClassifier(\n",
    "    RandomForestClassifier(n_estimators=150, class_weight='balanced',\n",
    "                           max_depth=10, min_samples_leaf=5, # From exploration/tuning\n",
    "                           random_state=SEED, n_jobs=-1)\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train_sel, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred_val_rf = rf_model.predict(X_val_sel)\n",
    "y_pred_adhd_val = y_pred_val_rf[:, 0]\n",
    "y_pred_sex_val = y_pred_val_rf[:, 1]\n",
    "y_true_adhd_val = y_val['ADHD_Outcome'].values\n",
    "y_true_sex_val = y_val['Sex_F'].values\n",
    "\n",
    "# Print Classification Reports\n",
    "print(f\"\\nValidation Report for ADHD_Outcome ({model_name}):\")\n",
    "print(classification_report(y_true_adhd_val, y_pred_adhd_val))\n",
    "print(f\"Validation Report for Sex_F ({model_name}):\")\n",
    "print(classification_report(y_true_sex_val, y_pred_sex_val))\n",
    "\n",
    "# Calculate and store metrics\n",
    "adhd_f1 = f1_score(y_true_adhd_val, y_pred_adhd_val)\n",
    "sex_f1 = f1_score(y_true_sex_val, y_pred_sex_val)\n",
    "adhd_acc = accuracy_score(y_true_adhd_val, y_pred_adhd_val)\n",
    "sex_acc = accuracy_score(y_true_sex_val, y_pred_sex_val)\n",
    "evaluation_results[model_name] = {\n",
    "    'ADHD_Accuracy': adhd_acc, 'ADHD_F1': adhd_f1,\n",
    "    'Sex_Accuracy': sex_acc, 'Sex_F1': sex_f1\n",
    "}\n",
    "print(f\"{model_name} - ADHD Accuracy: {adhd_acc:.4f}, F1-Score: {adhd_f1:.4f}\")\n",
    "print(f\"{model_name} - Sex Accuracy: {sex_acc:.4f}, F1-Score: {sex_f1:.4f}\")\n",
    "\n",
    "# Plot Confusion Matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "fig.suptitle(f'{model_name} - Validation Set Confusion Matrices')\n",
    "cm_adhd = confusion_matrix(y_true_adhd_val, y_pred_adhd_val)\n",
    "disp_adhd = ConfusionMatrixDisplay(confusion_matrix=cm_adhd, display_labels=[0, 1])\n",
    "disp_adhd.plot(ax=axes[0], cmap='Blues')\n",
    "axes[0].set_title('ADHD Outcome')\n",
    "cm_sex = confusion_matrix(y_true_sex_val, y_pred_sex_val)\n",
    "disp_sex = ConfusionMatrixDisplay(confusion_matrix=cm_sex, display_labels=[0, 1])\n",
    "disp_sex.plot(ax=axes[1], cmap='Greens')\n",
    "axes[1].set_title('Sex (Female)')\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n",
    "print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a12f2e2",
   "metadata": {},
   "source": [
    "### Neural Network (MLP) Model code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3f389f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Neural Network (MLP) ---\n",
    "model_name = 'NeuralNetwork'\n",
    "print(f\"\\n--- Training and Evaluating: {model_name} ---\")\n",
    "\n",
    "# Define the model\n",
    "nn_model = MultiOutputClassifier(\n",
    "    MLPClassifier(hidden_layer_sizes=(100, 50), activation='relu', solver='adam',\n",
    "                  alpha=0.001, learning_rate='adaptive', max_iter=500,\n",
    "                  random_state=SEED, early_stopping=True, validation_fraction=0.1) # Added early stopping\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "nn_model.fit(X_train_sel, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred_val_nn = nn_model.predict(X_val_sel)\n",
    "y_pred_adhd_val = y_pred_val_nn[:, 0]\n",
    "y_pred_sex_val = y_pred_val_nn[:, 1]\n",
    "y_true_adhd_val = y_val['ADHD_Outcome'].values\n",
    "y_true_sex_val = y_val['Sex_F'].values\n",
    "\n",
    "# Print Classification Reports\n",
    "print(f\"\\nValidation Report for ADHD_Outcome ({model_name}):\")\n",
    "print(classification_report(y_true_adhd_val, y_pred_adhd_val))\n",
    "print(f\"Validation Report for Sex_F ({model_name}):\")\n",
    "print(classification_report(y_true_sex_val, y_pred_sex_val))\n",
    "\n",
    "# Calculate and store metrics\n",
    "adhd_f1 = f1_score(y_true_adhd_val, y_pred_adhd_val)\n",
    "sex_f1 = f1_score(y_true_sex_val, y_pred_sex_val)\n",
    "adhd_acc = accuracy_score(y_true_adhd_val, y_pred_adhd_val)\n",
    "sex_acc = accuracy_score(y_true_sex_val, y_pred_sex_val)\n",
    "evaluation_results[model_name] = {\n",
    "    'ADHD_Accuracy': adhd_acc, 'ADHD_F1': adhd_f1,\n",
    "    'Sex_Accuracy': sex_acc, 'Sex_F1': sex_f1\n",
    "}\n",
    "print(f\"{model_name} - ADHD Accuracy: {adhd_acc:.4f}, F1-Score: {adhd_f1:.4f}\")\n",
    "print(f\"{model_name} - Sex Accuracy: {sex_acc:.4f}, F1-Score: {sex_f1:.4f}\")\n",
    "\n",
    "# Plot Confusion Matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "fig.suptitle(f'{model_name} - Validation Set Confusion Matrices')\n",
    "cm_adhd = confusion_matrix(y_true_adhd_val, y_pred_adhd_val)\n",
    "disp_adhd = ConfusionMatrixDisplay(confusion_matrix=cm_adhd, display_labels=[0, 1])\n",
    "disp_adhd.plot(ax=axes[0], cmap='Blues')\n",
    "axes[0].set_title('ADHD Outcome')\n",
    "cm_sex = confusion_matrix(y_true_sex_val, y_pred_sex_val)\n",
    "disp_sex = ConfusionMatrixDisplay(confusion_matrix=cm_sex, display_labels=[0, 1])\n",
    "disp_sex.plot(ax=axes[1], cmap='Greens')\n",
    "axes[1].set_title('Sex (Female)')\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n",
    "print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d14be5c",
   "metadata": {},
   "source": [
    "### Tuned Logistic Regression Model code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f0ca06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Tuned Logistic Regression ---\n",
    "model_name = 'LogisticRegression_Tuned'\n",
    "print(f\"\\n--- Training and Evaluating: {model_name} ---\")\n",
    "\n",
    "# Define the model using the best parameters found previously\n",
    "lr_tuned_model = MultiOutputClassifier(\n",
    "    LogisticRegression(\n",
    "        C=0.1, penalty='elasticnet', solver='saga', l1_ratio=0.5,\n",
    "        class_weight='balanced', random_state=SEED, max_iter=1500\n",
    "    ), n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "lr_tuned_model.fit(X_train_sel, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred_val_lr = lr_tuned_model.predict(X_val_sel)\n",
    "y_pred_adhd_val = y_pred_val_lr[:, 0]\n",
    "y_pred_sex_val = y_pred_val_lr[:, 1]\n",
    "y_true_adhd_val = y_val['ADHD_Outcome'].values\n",
    "y_true_sex_val = y_val['Sex_F'].values\n",
    "\n",
    "# Print Classification Reports\n",
    "print(f\"\\nValidation Report for ADHD_Outcome ({model_name}):\")\n",
    "print(classification_report(y_true_adhd_val, y_pred_adhd_val))\n",
    "print(f\"Validation Report for Sex_F ({model_name}):\")\n",
    "print(classification_report(y_true_sex_val, y_pred_sex_val))\n",
    "\n",
    "# Calculate and store metrics\n",
    "adhd_f1 = f1_score(y_true_adhd_val, y_pred_adhd_val)\n",
    "sex_f1 = f1_score(y_true_sex_val, y_pred_sex_val)\n",
    "adhd_acc = accuracy_score(y_true_adhd_val, y_pred_adhd_val)\n",
    "sex_acc = accuracy_score(y_true_sex_val, y_pred_sex_val)\n",
    "evaluation_results[model_name] = {\n",
    "    'ADHD_Accuracy': adhd_acc, 'ADHD_F1': adhd_f1,\n",
    "    'Sex_Accuracy': sex_acc, 'Sex_F1': sex_f1\n",
    "}\n",
    "print(f\"{model_name} - ADHD Accuracy: {adhd_acc:.4f}, F1-Score: {adhd_f1:.4f}\")\n",
    "print(f\"{model_name} - Sex Accuracy: {sex_acc:.4f}, F1-Score: {sex_f1:.4f}\")\n",
    "\n",
    "# Plot Confusion Matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "fig.suptitle(f'{model_name} - Validation Set Confusion Matrices')\n",
    "cm_adhd = confusion_matrix(y_true_adhd_val, y_pred_adhd_val)\n",
    "disp_adhd = ConfusionMatrixDisplay(confusion_matrix=cm_adhd, display_labels=[0, 1])\n",
    "disp_adhd.plot(ax=axes[0], cmap='Blues')\n",
    "axes[0].set_title('ADHD Outcome')\n",
    "cm_sex = confusion_matrix(y_true_sex_val, y_pred_sex_val)\n",
    "disp_sex = ConfusionMatrixDisplay(confusion_matrix=cm_sex, display_labels=[0, 1])\n",
    "disp_sex.plot(ax=axes[1], cmap='Greens')\n",
    "axes[1].set_title('Sex (Female)')\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Assign the best model (based on prior analysis) for retraining\n",
    "best_model = lr_tuned_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14e4a81",
   "metadata": {},
   "source": [
    "### Model Evaluaion Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7255daa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Summarize Evaluation Results ---\n",
    "print(\"\\n--- Summary of Model Performance on Validation Set ---\")\n",
    "results_df = pd.DataFrame(evaluation_results).T # Transpose for models as rows\n",
    "print(results_df)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# You can add comments here discussing which model performed best based on the results_df\n",
    "# e.g., \"Based on the validation F1-scores, LogisticRegression_Tuned appears to be the best performing model overall, especially for ADHD prediction.\"\n",
    "# The 'best_model' variable was already assigned to the tuned LR model in the previous step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f057da0",
   "metadata": {},
   "source": [
    "### Main Execution Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbe9853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Execution ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure base directory exists before proceeding (especially important for local execution)\n",
    "    if not os.path.exists(BASE_DATA_DIR):\n",
    "        print(f\"ERROR: Base data directory not found: {BASE_DATA_DIR}\")\n",
    "        print(\"Please ensure the folder path is correct in the configuration.\")\n",
    "    else:\n",
    "        # === Part 1: Data Loading and Initial Setup ===\n",
    "        print(\"--- Step 1: Loading Data ---\")\n",
    "        train_cat_raw, train_fcm_raw, train_quant_raw, train_targets_raw = load_dataset(\n",
    "            TRAIN_CAT_FILE, TRAIN_FCM_FILE, TRAIN_QUANT_FILE, TRAIN_SOLUTIONS_FILE\n",
    "        )\n",
    "        test_cat_raw, test_fcm_raw, test_quant_raw, _ = load_dataset(\n",
    "            TEST_CAT_FILE, TEST_FCM_FILE, TEST_QUANT_FILE\n",
    "        )\n",
    "        test_ids_final = test_cat_raw['participant_id'].copy() # Store test IDs\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        # === Part 2: EDA ===\n",
    "        # Note: The EDA code (plots, value counts etc.) should be in preceding cells.\n",
    "        # This block assumes EDA has been performed and reviewed.\n",
    "        print(\"--- Step 2: Exploratory Data Analysis (Assumed complete from previous cells) ---\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        # === Part 3: Preprocessing ===\n",
    "        print(\"--- Step 3: Preprocessing Pipeline ---\")\n",
    "        # 3a. PCA\n",
    "        train_fcm_pca_df, test_fcm_pca_df, pca_model = apply_pca_reduction(\n",
    "            train_fcm_raw, test_fcm_raw, PCA_EXPLAINED_VARIANCE, SEED\n",
    "        )\n",
    "        # 3b. Encoding\n",
    "        train_cat_encoded_df, test_cat_encoded_df, train_encoded_cols = encode_categorical_features(\n",
    "            train_cat_raw, test_cat_raw\n",
    "        )\n",
    "        # 3c. Merging\n",
    "        train_merged = merge_all_data(train_cat_encoded_df, train_fcm_pca_df, train_quant_raw, train_targets_raw)\n",
    "        test_merged = merge_all_data(test_cat_encoded_df, test_fcm_pca_df, test_quant_raw)\n",
    "        # 3d. Imputation\n",
    "        train_imputed, train_means_used, train_modes_used = impute_missing_values(train_merged)\n",
    "        test_imputed, _, _ = impute_missing_values(test_merged, train_means=train_means_used, train_modes=train_modes_used)\n",
    "        # 3e. Column Alignment\n",
    "        train_feature_cols = train_imputed.drop(columns=['participant_id', 'ADHD_Outcome', 'Sex_F']).columns\n",
    "        test_id_col = test_imputed['participant_id']\n",
    "        test_imputed = test_imputed.reindex(columns=list(train_feature_cols), fill_value=0)\n",
    "        test_imputed.insert(0, 'participant_id', test_id_col)\n",
    "        print(\"Preprocessing complete.\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        # === Part 4: Feature/Target Preparation and Splitting ===\n",
    "        print(\"--- Step 4: Preparing Features/Targets & Splitting ---\")\n",
    "        X_all = train_imputed.drop(columns=['participant_id', 'ADHD_Outcome', 'Sex_F'])\n",
    "        y_all = train_imputed[['ADHD_Outcome', 'Sex_F']]\n",
    "        X_test_prepared = test_imputed.drop(columns=['participant_id'])\n",
    "        X_all.columns = X_all.columns.astype(str)\n",
    "        X_test_prepared.columns = X_test_prepared.columns.astype(str)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_all, y_all, test_size=VALIDATION_SPLIT_RATIO, random_state=SEED, stratify=y_all['Sex_F']\n",
    "        )\n",
    "        print(f\"Data split complete. Train shape: {X_train.shape}, Val shape: {X_val.shape}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        # === Part 5: Scaling and Feature Selection ===\n",
    "        print(\"--- Step 5: Scaling and Feature Selection ---\")\n",
    "        X_train_s, X_val_s, X_test_s, scaler_model = scale_data(X_train.values, X_val.values, X_test_prepared.values)\n",
    "        X_train_sel, X_val_sel, X_test_sel, feature_selection_mask = perform_feature_selection(\n",
    "            X_train_s, y_train, NUM_FEATURES_TO_SELECT, X_val_s, X_test_s\n",
    "        )\n",
    "        print(\"Scaling and Feature Selection complete.\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        # === Part 6: Model Exploration and Evaluation ===\n",
    "        # Note: The actual training/evaluation code for KNN, DT, RF, NN, Tuned LR\n",
    "        # should be in the preceding cells (as provided before).\n",
    "        # This block assumes those cells have been run and 'evaluation_results' dict is populated.\n",
    "        print(\"--- Step 6: Model Exploration & Evaluation (Assumed complete from previous cells) ---\")\n",
    "        # Display summary table again if desired\n",
    "        if 'evaluation_results' in locals():\n",
    "             results_df = pd.DataFrame(evaluation_results).T\n",
    "             print(\"\\n--- Summary of Model Performance on Validation Set ---\")\n",
    "             print(results_df)\n",
    "        else:\n",
    "             print(\"Evaluation results dictionary not found. Run model exploration cells.\")\n",
    "        # Assume 'best_model' variable (e.g., lr_tuned_model) was assigned in the exploration section\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "\n",
    "        # === Part 7: Retrain Best Model on Full Training Data ===\n",
    "        print(\"--- Step 7: Retraining Best Model on Full Training Data ---\")\n",
    "        # 7a. Scale the *entire* original training feature set (X_all)\n",
    "        print(f\"Scaling full training data (shape: {X_all.shape})...\")\n",
    "        X_all_scaled = scaler_model.transform(X_all.values) # Use transform\n",
    "\n",
    "        # 7b. Apply the *same* feature selection mask\n",
    "        print(f\"Applying feature selection mask (selecting {np.sum(feature_selection_mask)} features)...\")\n",
    "        X_all_selected = X_all_scaled[:, feature_selection_mask]\n",
    "        print(f\"Shape after scaling and selection: {X_all_selected.shape}\")\n",
    "\n",
    "        # 7c. Define and Train the best model architecture again on ALL training data\n",
    "        retrain_base_classifier = LogisticRegression(\n",
    "            C=0.1, penalty='elasticnet', solver='saga', l1_ratio=0.5,\n",
    "            class_weight='balanced', random_state=SEED, max_iter=1500\n",
    "        )\n",
    "        retrained_model = MultiOutputClassifier(retrain_base_classifier, n_jobs=-1)\n",
    "        print(\"Training model on full training dataset...\")\n",
    "        retrained_model.fit(X_all_selected, y_all) # Train on X_all_selected and y_all\n",
    "        print(\"Retraining complete.\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        # === Part 8: Generate Final Submission File ===\n",
    "        print(\"--- Step 8: Generating Final Submission File ---\")\n",
    "        # Use the retrained model and the processed TEST set features\n",
    "        generate_submission(retrained_model, X_test_sel, test_ids_final, SUBMISSION_FILE_PATH)\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        print(\"\\nPipeline finished.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
